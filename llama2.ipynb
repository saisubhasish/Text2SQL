{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dab8654e-0819-4388-bee0-56dd67b9ba64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# installing sagemaker\n",
    "\n",
    "!pip install \"sagemaker>=2.175.0\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cd2d970-8ff1-4ef8-9b7b-56d675e632b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::851725506990:role/service-role/AmazonSageMakerServiceCatalogProductsExecutionRole\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# sagemaker session bucket -> used for uploading data, models, and logs\n",
    "# samaker will automatically create this bucket if it not exists\n",
    "# creating sagemaker session bucket\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket, if a bucketname is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "    \n",
    "# set role\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn'] # IAM role for sagemake execution\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket) # sagemaker session with bucket\n",
    "\n",
    "# printing role and region\n",
    "print(f'sagemaker role arn: {role}')\n",
    "print(f'sagemaker session region: {sess.boto_region_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "844001e9-130f-4c84-a11e-0e309db73897",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm image uri: 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi0.9.3-gpu-py39-cu118-ubuntu20.04\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# retrieve the llm image uri from huggingface llm image\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "    \"huggingface\",\n",
    "    version='0.9.3'\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f'llm image uri: {llm_image}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8710f39d-9ed9-401a-8d81-f3e5de99fc8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "number_of_gpu = 1\n",
    "health_check_timeout = 300\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"meta-llama/Llama-2-7b-chat-hf\", # model_id from hf.co/models\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(2048),  # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(4096),  # Max length of the generation (including input text)\n",
    "  'MAX_BATCH_TOTAL_TOKENS': json.dumps(8192),  # Limits the number of tokens that can be processed in parallel during the generation\n",
    "  'HUGGING_FACE_HUB_TOKEN': json.dumps(\"hf_GmuoRVpKcKxqxYPGsYXTTQicwVeOBpIgGpu\")\n",
    "}\n",
    "\n",
    "# check if token is set\n",
    "assert config['HUGGING_FACE_HUB_TOKEN'] != \"hf_GmuoRVpKcKxqxYPGsYXTTQicwVeOBpIgGpu\", \"Please set your Hugging Face Hub Token\"\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  env=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd6d6ce6-90ac-49a1-a030-ecd753fa39f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------!"
     ]
    }
   ],
   "source": [
    "# Deploy mode to endpoint\n",
    "\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to be able to load the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba77acd7-32e5-4d56-b28b-d99ee7fe7ced",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# building prompt for llama2 based on conversation history\n",
    "\n",
    "def build_llama2_prompt(messages):\n",
    "    startPrompt = \"<s>[INST] \"\n",
    "    endPrompt = \" [/INST]\"\n",
    "    conversation = []\n",
    "    for index, message in enumerate(messages):\n",
    "        if message[\"role\"] == \"system\" and index == 0:\n",
    "            conversation.append(f\"<<SYS>>\\n{message['content']}\\n<</SYS>>\\n\\n\")\n",
    "        elif message[\"role\"] == \"user\":\n",
    "            conversation.append(message[\"content\"].strip())\n",
    "        else:\n",
    "            conversation.append(f\" [/INST] {message['content'].strip()}</s><s>[INST]\")\n",
    "\n",
    "    return startPrompt + \"\".join(conversation) + endPrompt\n",
    "\n",
    "messages = [\n",
    "{\"role\": \"system\", \"content\": \"You are an advanced Text-to-SQL converter specializing in SQLite3 queries. Your task is to translate natural language English input into accurate and efficient SQL queries.\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8152633-793a-49e3-8dad-8272d18d0fa5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello there! I'm Nyara, and I'm here to help you understand the\n"
     ]
    }
   ],
   "source": [
    "# sending prompt to model for prediction\n",
    "\n",
    "instruction = \"What is quantum computing?\"\n",
    "messages.append({\"role\": \"user\", \"content\": instruction})\n",
    "prompt = build_llama2_prompt(messages)\n",
    "\n",
    "chat = llm.predict({\"inputs\": prompt})\n",
    "\n",
    "print(chat[0]['generated_text'][len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e0df38e-2631-489b-a828-4ac3dae284c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# detailed payload to control the generation process\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"do_sample\":True,\n",
    "        \"top_p\":0.6,\n",
    "        \"temperatrure\":0.8,\n",
    "        'top_k':50,\n",
    "        \"max_new_tokens\":512,\n",
    "        \"repetition_penalty\":1.03,\n",
    "        \"stop\":[\"</s>\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47c362c3-f9f7-4d30-83fa-9257aa370e57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = llm.predict(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0409c21-b4f1-45f0-bbc7-800502f894c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello there! I'm Nyara, and I'm here to help you understand the fascinating world of quantum computing! üöÄ\n",
      "\n",
      "Quantum computing is a revolutionary field that leverages the principles of quantum mechanics to perform complex calculations and simulations that are beyond the capabilities of classical computers. In classical computing, information is processed using bits that can have a value of either 0 or 1. In contrast, quantum computing uses quantum bits, or qubits, which can exist in multiple states simultaneously, known as superposition. üí°\n",
      "\n",
      "This property allows quantum computers to solve certain problems much faster than classical computers, which are limited by the laws of classical physics. Quantum computers can also entangle qubits, which enables them to perform calculations on multiple variables simultaneously, known as parallel processing. üîç\n",
      "\n",
      "Quantum computing has the potential to solve some of the most challenging problems in various fields, including:\n",
      "\n",
      "1. Cryptography: Quantum computers can break many encryption algorithms currently in use, but they can also be used to create new, quantum-resistant encryption methods. üîí\n",
      "2. Optimization: Quantum computers can quickly find the optimal solution for complex optimization problems, such as portfolio optimization in finance or logistics optimization in supply chain management. üìà\n",
      "3. Material Science: Quantum computers can simulate the behavior of materials at the atomic level, which can lead to breakthroughs in the development of new materials and drugs. üß¨\n",
      "4. Machine Learning: Quantum computers can train machine learning models much faster than classical computers, which can lead to advancements in areas like natural language processing and image recognition. ü§ñ\n",
      "\n",
      "While quantum computing is still in its early stages, it has the potential to revolutionize many fields and industries. However, building and programming quantum computers is a complex task, and significant research is being done to develop the necessary hardware and software. üíª\n",
      "\n",
      "I hope this helps you understand the basics of quantum computing! Do you have any specific questions or topics you'd like to explore further? ü§î\n"
     ]
    }
   ],
   "source": [
    "print(response[0]['generated_text'][len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8a7ee5-b4e6-44dd-b41a-5faa0cd693e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
